{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edf94762",
        "outputId": "257b9562-0bde-4402-c8b2-5b2652b0651e"
      },
      "source": [
        "%pip install ibm_watsonx_ai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ibm_watsonx_ai in /usr/local/lib/python3.11/dist-packages (1.3.32)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (2.5.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (2.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (2025.7.14)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (25.0)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (2.14.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from ibm_watsonx_ai) (5.5.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm_watsonx_ai) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm_watsonx_ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm_watsonx_ai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm_watsonx_ai) (0.16.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.14.3 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm_watsonx_ai) (2.14.3)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.3 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm_watsonx_ai) (2.14.3)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm_watsonx_ai) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.14.3->ibm-cos-sdk<2.15.0,>=2.12.0->ibm_watsonx_ai) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm_watsonx_ai) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm_watsonx_ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm_watsonx_ai) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ibm_watsonx_ai) (3.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm_watsonx_ai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ibm_watsonx_ai) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ibm_watsonx_ai) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cde0faf1",
        "outputId": "f16d0c96-1441-40e8-826c-ab01abc89f3e"
      },
      "source": [
        "%pip install langchain-ibm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-ibm in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
            "Requirement already satisfied: ibm-watsonx-ai<2.0.0,>=1.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-ibm) (1.3.32)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.39 in /usr/local/lib/python3.11/dist-packages (from langchain-ibm) (0.3.72)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.5.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2025.7.14)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (25.0)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.14.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (5.5.2)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.4.9)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (4.14.1)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (0.16.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.14.3 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.14.3)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.3 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.14.3)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.14.3->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.9.0.post0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (3.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.28->langchain-ibm) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDqArM3XN5Q"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
        "# AI Service Deployment Notebook\n",
        "This notebook contains steps and code to test, promote, and deploy an Agent as an AI Service.\n",
        "\n",
        "**Note:** Notebook code generated using Agent Lab will execute successfully.\n",
        "If code is modified or reordered, there is no guarantee it will successfully execute.\n",
        "For details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n",
        "\n",
        "\n",
        "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
        "\n",
        "## Contents\n",
        "This notebook contains the following parts:\n",
        "\n",
        "1. Setup\n",
        "2. Initialize all the variables needed by the AI Service\n",
        "3. Define the AI service function\n",
        "4. Deploy an AI Service\n",
        "5. Test the deployed AI Service\n",
        "\n",
        "## 1. Set up the environment\n",
        "\n",
        "Before you can run this notebook, you must perform the following setup tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXaEVi14XN5R"
      },
      "source": [
        "### Connection to WML\n",
        "This cell defines the credentials required to work with watsonx API for both the execution in the project,\n",
        "as well as the deployment and runtime execution of the function.\n",
        "\n",
        "**Action:** Provide the IBM Cloud personal API key. For details, see\n",
        "<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y18dD6ZXN5S",
        "outputId": "e4fc4982-4888-4614-d8bb-082d6fba5068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your api key (hit enter): ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from ibm_watsonx_ai import APIClient, Credentials\n",
        "import getpass\n",
        "\n",
        "credentials = Credentials(\n",
        "    url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
        "    api_key=getpass.getpass(\"Please enter your api key (hit enter): \")\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R_5uH1IkXN5T"
      },
      "outputs": [],
      "source": [
        "client = APIClient(credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s12aQSTPXN5T"
      },
      "source": [
        "### Connecting to a space\n",
        "A space will be be used to host the promoted AI Service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jzlYIqihXN5T",
        "outputId": "9d61de1e-e28c-490b-e3f8-a125c65faa66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SUCCESS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "space_id = \"2aed579a-be8b-48ed-9821-8dee476ba45d\"\n",
        "client.set.default_space(space_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2arvQ_nXN5T"
      },
      "source": [
        "### Promote asset(s) to space\n",
        "We will now promote assets we will need to stage in the space so that we can access their data from the AI service.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5mHLuW0PXN5T"
      },
      "outputs": [],
      "source": [
        "source_project_id = \"f1a2cb03-a491-4534-b4b1-952e44095281\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crq1xFVGXN5T"
      },
      "source": [
        "## 2. Create the AI service function\n",
        "We first need to define the AI service function\n",
        "\n",
        "### 2.1 Define the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fJtsfHCYXN5T"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"space_id\": space_id,\n",
        "}\n",
        "\n",
        "def gen_ai_service(context, params = params, **custom):\n",
        "    # import dependencies\n",
        "    from langchain_ibm import ChatWatsonx\n",
        "    from ibm_watsonx_ai import APIClient\n",
        "    from ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\n",
        "    from langchain_core.messages import AIMessage, HumanMessage\n",
        "    from langgraph.checkpoint.memory import MemorySaver\n",
        "    from langgraph.prebuilt import create_react_agent\n",
        "    import json\n",
        "    import requests\n",
        "\n",
        "    model = \"meta-llama/llama-3-3-70b-instruct\"\n",
        "\n",
        "    service_url = \"https://eu-gb.ml.cloud.ibm.com\"\n",
        "    # Get credentials token\n",
        "    credentials = {\n",
        "        \"url\": service_url,\n",
        "        \"token\": context.generate_token()\n",
        "    }\n",
        "\n",
        "    # Setup client\n",
        "    client = APIClient(credentials)\n",
        "    space_id = params.get(\"space_id\")\n",
        "    client.set.default_space(space_id)\n",
        "\n",
        "\n",
        "\n",
        "    def create_chat_model(watsonx_client):\n",
        "        parameters = {\n",
        "            \"frequency_penalty\": 0.05,\n",
        "            \"max_tokens\": 4000,\n",
        "            \"presence_penalty\": 0.42,\n",
        "            \"temperature\": 0.26,\n",
        "            \"top_p\": 0.99,\n",
        "            \"seed\": 10\n",
        "        }\n",
        "\n",
        "        chat_model = ChatWatsonx(\n",
        "            model_id=model,\n",
        "            url=service_url,\n",
        "            space_id=space_id,\n",
        "            params=parameters,\n",
        "            watsonx_client=watsonx_client,\n",
        "        )\n",
        "        return chat_model\n",
        "\n",
        "\n",
        "    def create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n",
        "        from langchain_core.tools import StructuredTool\n",
        "        utility_agent_tool = Toolkit(\n",
        "            api_client=api_client\n",
        "        ).get_tool(tool_name)\n",
        "\n",
        "        tool_description = utility_agent_tool.get(\"description\")\n",
        "\n",
        "        if (kwargs.get(\"tool_description\")):\n",
        "            tool_description = kwargs.get(\"tool_description\")\n",
        "        elif (utility_agent_tool.get(\"agent_description\")):\n",
        "            tool_description = utility_agent_tool.get(\"agent_description\")\n",
        "\n",
        "        tool_schema = utility_agent_tool.get(\"input_schema\")\n",
        "        if (tool_schema == None):\n",
        "            tool_schema = {\n",
        "                \"type\": \"object\",\n",
        "                \"additionalProperties\": False,\n",
        "                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "                \"properties\": {\n",
        "                    \"input\": {\n",
        "                        \"description\": \"input for the tool\",\n",
        "                        \"type\": \"string\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "        def run_tool(**tool_input):\n",
        "            query = tool_input\n",
        "            if (utility_agent_tool.get(\"input_schema\") == None):\n",
        "                query = tool_input.get(\"input\")\n",
        "\n",
        "            results = utility_agent_tool.run(\n",
        "                input=query,\n",
        "                config=params\n",
        "            )\n",
        "\n",
        "            return results.get(\"output\")\n",
        "\n",
        "        return StructuredTool(\n",
        "            name=tool_name,\n",
        "            description = tool_description,\n",
        "            func=run_tool,\n",
        "            args_schema=tool_schema\n",
        "        )\n",
        "\n",
        "\n",
        "    def create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n",
        "        from langchain_core.tools import StructuredTool\n",
        "        import ast\n",
        "\n",
        "        def call_tool(**kwargs):\n",
        "            tree = ast.parse(tool_code, mode=\"exec\")\n",
        "            custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n",
        "            function_name = custom_tool_functions[0].name\n",
        "            compiled_code = compile(tree, 'custom_tool', 'exec')\n",
        "            namespace = tool_params if tool_params else {}\n",
        "            exec(compiled_code, namespace)\n",
        "            return namespace[function_name](**kwargs)\n",
        "\n",
        "        tool = StructuredTool(\n",
        "            name=tool_name,\n",
        "            description = tool_description,\n",
        "            func=call_tool,\n",
        "            args_schema=tool_schema\n",
        "        )\n",
        "        return tool\n",
        "\n",
        "    def create_custom_tools():\n",
        "        custom_tools = []\n",
        "\n",
        "\n",
        "    def create_tools(inner_client, context):\n",
        "        tools = []\n",
        "\n",
        "        config = None\n",
        "        tools.append(create_utility_agent_tool(\"GoogleSearch\", config, inner_client))\n",
        "        config = {\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, inner_client))\n",
        "        config = {\n",
        "            \"maxResults\": 5\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"Wikipedia\", config, inner_client))\n",
        "        config = {\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"Weather\", config, inner_client))\n",
        "        config = {\n",
        "        }\n",
        "        tools.append(create_utility_agent_tool(\"WebCrawler\", config, inner_client))\n",
        "        return tools\n",
        "\n",
        "    def create_agent(model, tools, messages):\n",
        "        memory = MemorySaver()\n",
        "        instructions = \"\"\"# Notes\n",
        "- Use markdown syntax for formatting code snippets, links, JSON, tables, images, files.\n",
        "- Any HTML tags must be wrapped in block quotes, for example ```<html>```.\n",
        "- When returning code blocks, specify language.\n",
        "- Sometimes, things don't go as planned. Tools may not provide useful information on the first few tries. You should always try a few different approaches before declaring the problem unsolvable.\n",
        "- When the tool doesn't give you what you were asking for, you must either use another tool or a different tool input.\n",
        "- When using search engines, you try different formulations of the query, possibly even in a different language.\n",
        "- You cannot do complex calculations, computations, or data manipulations without using tools.\n",
        "- If you need to call a tool to compute something, always call it instead of saying you will call it.\n",
        "\n",
        "If a tool returns an IMAGE in the result, you must include it in your answer as Markdown.\n",
        "\n",
        "Example:\n",
        "\n",
        "Tool result: IMAGE({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n",
        "Markdown to return to user: ![Generated image]({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n",
        "\n",
        "You are a helpful assistant who utilizes tools to provide detailed answers to questions. When greeted, say, \\\"Hi, I am an AI Interview Partner agent. How can I help you today?\\\"\"\"\"\n",
        "        for message in messages:\n",
        "            if message[\"role\"] == \"system\":\n",
        "                instructions += message[\"content\"]\n",
        "        graph = create_react_agent(model, tools=tools, checkpointer=memory, state_modifier=instructions)\n",
        "        return graph\n",
        "\n",
        "    def convert_messages(messages):\n",
        "        converted_messages = []\n",
        "        for message in messages:\n",
        "            if (message[\"role\"] == \"user\"):\n",
        "                converted_messages.append(HumanMessage(content=message[\"content\"]))\n",
        "            elif (message[\"role\"] == \"assistant\"):\n",
        "                converted_messages.append(AIMessage(content=message[\"content\"]))\n",
        "        return converted_messages\n",
        "\n",
        "    def generate(context):\n",
        "        payload = context.get_json()\n",
        "        messages = payload.get(\"messages\")\n",
        "        inner_credentials = {\n",
        "            \"url\": service_url,\n",
        "            \"token\": context.get_token()\n",
        "        }\n",
        "\n",
        "        inner_client = APIClient(inner_credentials)\n",
        "        model = create_chat_model(inner_client)\n",
        "        tools = create_tools(inner_client, context)\n",
        "        agent = create_agent(model, tools, messages)\n",
        "\n",
        "        generated_response = agent.invoke(\n",
        "            { \"messages\": convert_messages(messages) },\n",
        "            { \"configurable\": { \"thread_id\": \"42\" } }\n",
        "        )\n",
        "\n",
        "        last_message = generated_response[\"messages\"][-1]\n",
        "        generated_response = last_message.content\n",
        "\n",
        "        execute_response = {\n",
        "            \"headers\": {\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            },\n",
        "            \"body\": {\n",
        "                \"choices\": [{\n",
        "                    \"index\": 0,\n",
        "                    \"message\": {\n",
        "                       \"role\": \"assistant\",\n",
        "                       \"content\": generated_response\n",
        "                    }\n",
        "                }]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return execute_response\n",
        "\n",
        "    def generate_stream(context):\n",
        "        print(\"Generate stream\", flush=True)\n",
        "        payload = context.get_json()\n",
        "        headers = context.get_headers()\n",
        "        is_assistant = headers.get(\"X-Ai-Interface\") == \"assistant\"\n",
        "        messages = payload.get(\"messages\")\n",
        "        inner_credentials = {\n",
        "            \"url\": service_url,\n",
        "            \"token\": context.get_token()\n",
        "        }\n",
        "        inner_client = APIClient(inner_credentials)\n",
        "        model = create_chat_model(inner_client)\n",
        "        tools = create_tools(inner_client, context)\n",
        "        agent = create_agent(model, tools, messages)\n",
        "\n",
        "        response_stream = agent.stream(\n",
        "            { \"messages\": messages },\n",
        "            { \"configurable\": { \"thread_id\": \"42\" } },\n",
        "            stream_mode=[\"updates\", \"messages\"]\n",
        "        )\n",
        "\n",
        "        for chunk in response_stream:\n",
        "            chunk_type = chunk[0]\n",
        "            finish_reason = \"\"\n",
        "            usage = None\n",
        "            if (chunk_type == \"messages\"):\n",
        "                message_object = chunk[1][0]\n",
        "                if (message_object.type == \"AIMessageChunk\" and message_object.content != \"\"):\n",
        "                    message = {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": message_object.content\n",
        "                    }\n",
        "                else:\n",
        "                    continue\n",
        "            elif (chunk_type == \"updates\"):\n",
        "                update = chunk[1]\n",
        "                if (\"agent\" in update):\n",
        "                    agent = update[\"agent\"]\n",
        "                    agent_result = agent[\"messages\"][0]\n",
        "                    if (agent_result.additional_kwargs):\n",
        "                        kwargs = agent[\"messages\"][0].additional_kwargs\n",
        "                        tool_call = kwargs[\"tool_calls\"][0]\n",
        "                        if (is_assistant):\n",
        "                            message = {\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"step_details\": {\n",
        "                                    \"type\": \"tool_calls\",\n",
        "                                    \"tool_calls\": [\n",
        "                                        {\n",
        "                                            \"id\": tool_call[\"id\"],\n",
        "                                            \"name\": tool_call[\"function\"][\"name\"],\n",
        "                                            \"args\": tool_call[\"function\"][\"arguments\"]\n",
        "                                        }\n",
        "                                    ]\n",
        "                                }\n",
        "                            }\n",
        "                        else:\n",
        "                            message = {\n",
        "                                \"role\": \"assistant\",\n",
        "                                \"tool_calls\": [\n",
        "                                    {\n",
        "                                        \"id\": tool_call[\"id\"],\n",
        "                                        \"type\": \"function\",\n",
        "                                        \"function\": {\n",
        "                                            \"name\": tool_call[\"function\"][\"name\"],\n",
        "                                            \"arguments\": tool_call[\"function\"][\"arguments\"]\n",
        "                                        }\n",
        "                                    }\n",
        "                                ]\n",
        "                            }\n",
        "                    elif (agent_result.response_metadata):\n",
        "                        # Final update\n",
        "                        message = {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"content\": agent_result.content\n",
        "                        }\n",
        "                        finish_reason = agent_result.response_metadata[\"finish_reason\"]\n",
        "                        if (finish_reason):\n",
        "                            message[\"content\"] = \"\"\n",
        "\n",
        "                        usage = {\n",
        "                            \"completion_tokens\": agent_result.usage_metadata[\"output_tokens\"],\n",
        "                            \"prompt_tokens\": agent_result.usage_metadata[\"input_tokens\"],\n",
        "                            \"total_tokens\": agent_result.usage_metadata[\"total_tokens\"]\n",
        "                        }\n",
        "                elif (\"tools\" in update):\n",
        "                    tools = update[\"tools\"]\n",
        "                    tool_result = tools[\"messages\"][0]\n",
        "                    if (is_assistant):\n",
        "                        message = {\n",
        "                            \"role\": \"assistant\",\n",
        "                            \"step_details\": {\n",
        "                                \"type\": \"tool_response\",\n",
        "                                \"id\": tool_result.id,\n",
        "                                \"tool_call_id\": tool_result.tool_call_id,\n",
        "                                \"name\": tool_result.name,\n",
        "                                \"content\": tool_result.content\n",
        "                            }\n",
        "                        }\n",
        "                    else:\n",
        "                        message = {\n",
        "                            \"role\": \"tool\",\n",
        "                            \"id\": tool_result.id,\n",
        "                            \"tool_call_id\": tool_result.tool_call_id,\n",
        "                            \"name\": tool_result.name,\n",
        "                            \"content\": tool_result.content\n",
        "                        }\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            chunk_response = {\n",
        "                \"choices\": [{\n",
        "                    \"index\": 0,\n",
        "                    \"delta\": message\n",
        "                }]\n",
        "            }\n",
        "            if (finish_reason):\n",
        "                chunk_response[\"choices\"][0][\"finish_reason\"] = finish_reason\n",
        "            if (usage):\n",
        "                chunk_response[\"usage\"] = usage\n",
        "            yield chunk_response\n",
        "\n",
        "    return generate, generate_stream\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4rNID8tXN5U"
      },
      "source": [
        "### 2.2 Test locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "el2xCzUfXN5U"
      },
      "outputs": [],
      "source": [
        "# Initialize AI Service function locally\n",
        "from ibm_watsonx_ai.deployments import RuntimeContext\n",
        "\n",
        "context = RuntimeContext(api_client=client)\n",
        "\n",
        "streaming = False\n",
        "findex = 1 if streaming else 0\n",
        "local_function = gen_ai_service(context,  space_id=space_id)[findex]\n",
        "messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "rPWFBkagXN5U",
        "outputId": "7b7cde08-59ad-4c25-da90-97f0590522f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ibm_watsonx_ai.wml_client_error:Failure during chat. (POST https://eu-gb.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-16)\n",
            "Status code: 429, body: {\"errors\":[{\"code\":\"consumption_limit_reached\",\"message\":\"The usage limit for the current plan has been reached: the total number of free concurrent requests for model meta-llama/llama-3-3-70b-instruct has reached its limit 10. Please try again later\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\"}],\"trace\":\"8e4d3722999a606b8efbebf11485a6cf\",\"status_code\":429}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ApiRequestFailure",
          "evalue": "Failure during chat. (POST https://eu-gb.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-16)\nStatus code: 429, body: {\"errors\":[{\"code\":\"consumption_limit_reached\",\"message\":\"The usage limit for the current plan has been reached: the total number of free concurrent requests for model meta-llama/llama-3-3-70b-instruct has reached its limit 10. Please try again later\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\"}],\"trace\":\"8e4d3722999a606b8efbebf11485a6cf\",\"status_code\":429}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3498416419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRuntimeContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_payload_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2082195450.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         generated_response = agent.invoke(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;34m{\u001b[0m \u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconvert_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;34m{\u001b[0m \u001b[0;34m\"configurable\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"thread_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"42\"\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3015\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3016\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2640\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2641\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2642\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2643\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/prebuilt/chat_agent_executor.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, runtime, config)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# add agent name to the AIMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3045\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3046\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3047\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5432\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5433\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5434\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5435\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m         return cast(\n\u001b[1;32m    394\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    979\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 results.append(\n\u001b[0;32m--> 799\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    800\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1046\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_ibm/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m             )\n\u001b[1;32m    801\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             response = self.watsonx_model.chat(\n\u001b[0m\u001b[1;32m    803\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupdated_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/foundation_models/inference/model_inference.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, messages, params, tools, tool_choice, tool_choice_option, context)\u001b[0m\n\u001b[1;32m    345\u001b[0m             )\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         return self._inference.chat(\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, messages, params, tools, tool_choice, tool_choice_option, context)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mtext_chat_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_href_definitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fm_chat_href\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         return self._send_chat_payload(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py\u001b[0m in \u001b[0;36m_send_chat_payload\u001b[0;34m(self, messages, params, generate_url, tools, tool_choice, tool_choice_option)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mresponse_scoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         return self._handle_response(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;34m\"chat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/wml_resource.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response, _silent_response_logging, _field_to_hide)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             raise ApiRequestFailure(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"Failure during {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperationName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during chat. (POST https://eu-gb.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-16)\nStatus code: 429, body: {\"errors\":[{\"code\":\"consumption_limit_reached\",\"message\":\"The usage limit for the current plan has been reached: the total number of free concurrent requests for model meta-llama/llama-3-3-70b-instruct has reached its limit 10. Please try again later\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\"}],\"trace\":\"8e4d3722999a606b8efbebf11485a6cf\",\"status_code\":429}"
          ]
        }
      ],
      "source": [
        "local_question = \"Change this question to test your function\"\n",
        "\n",
        "messages.append({ \"role\" : \"user\", \"content\": local_question })\n",
        "\n",
        "context = RuntimeContext(api_client=client, request_payload_json={\"messages\": messages})\n",
        "\n",
        "response = local_function(context)\n",
        "\n",
        "result = ''\n",
        "\n",
        "if (streaming):\n",
        "    for chunk in response:\n",
        "        print(chunk, end=\"\\n\\n\", flush=True)\n",
        "else:\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKV3eg42XN5U"
      },
      "source": [
        "## 3. Store and deploy the AI Service\n",
        "Before you can deploy the AI Service, you must store the AI service in your watsonx.ai repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vnqgcCUGXN5U"
      },
      "outputs": [],
      "source": [
        "# Look up software specification for the AI service\n",
        "software_spec_id_in_project = \"45f12dfe-aa78-5b8d-9f38-0ee223c47309\"\n",
        "software_spec_id = \"\"\n",
        "\n",
        "try:\n",
        "    software_spec_id = client.software_specifications.get_id_by_name(\"runtime-24.1-py3.11\")\n",
        "except:\n",
        "    software_spec_id = client.spaces.promote(software_spec_id_in_project, source_project_id, space_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5p61BpCJXN5U"
      },
      "outputs": [],
      "source": [
        "# Define the request and response schemas for the AI service\n",
        "request_schema = {\n",
        "    \"application/json\": {\n",
        "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"messages\": {\n",
        "                \"title\": \"The messages for this chat session.\",\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"role\": {\n",
        "                            \"title\": \"The role of the message author.\",\n",
        "                            \"type\": \"string\",\n",
        "                            \"enum\": [\"user\",\"assistant\"]\n",
        "                        },\n",
        "                        \"content\": {\n",
        "                            \"title\": \"The contents of the message.\",\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"role\",\"content\"]\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"messages\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "response_schema = {\n",
        "    \"application/json\": {\n",
        "        \"oneOf\": [{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service_stream\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices.\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"title\":\"The index of this result.\"},\"delta\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"content\":{\"description\":\"The contents of the message.\",\"type\":\"string\"},\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]},{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"description\":\"The index of this result.\"},\"message\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"},\"content\":{\"title\":\"Message content.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]}]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MAG_B2OnXN5V"
      },
      "outputs": [],
      "source": [
        "# Store the AI service in the repository\n",
        "ai_service_metadata = {\n",
        "    client.repository.AIServiceMetaNames.NAME: \"Interview partner buddy\",\n",
        "    client.repository.AIServiceMetaNames.DESCRIPTION: \"\",\n",
        "    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: software_spec_id,\n",
        "    client.repository.AIServiceMetaNames.CUSTOM: {},\n",
        "    client.repository.AIServiceMetaNames.REQUEST_DOCUMENTATION: request_schema,\n",
        "    client.repository.AIServiceMetaNames.RESPONSE_DOCUMENTATION: response_schema,\n",
        "    client.repository.AIServiceMetaNames.TAGS: [\"wx-agent\"]\n",
        "}\n",
        "\n",
        "ai_service_details = client.repository.store_ai_service(meta_props=ai_service_metadata, ai_service=gen_ai_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HI4Kn0nAXN5V"
      },
      "outputs": [],
      "source": [
        "# Get the AI Service ID\n",
        "\n",
        "ai_service_id = client.repository.get_ai_service_id(ai_service_details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjYbPpi1XN5V",
        "outputId": "6276d290-5d2e-4f9a-ebbc-98e859a05f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "######################################################################################\n",
            "\n",
            "Synchronous deployment creation for id: '0e60a056-0729-481e-bf62-2a75f2367e51' started\n",
            "\n",
            "######################################################################################\n",
            "\n",
            "\n",
            "initializing\n",
            "Note: online_url and serving_urls are deprecated and will be removed in a future release. Use inference instead.\n",
            ".....\n",
            "ready\n",
            "\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "Successfully finished deployment creation, deployment_id='0c52f4ed-456f-474d-85e2-bd78f381c8ec'\n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Deploy the stored AI Service\n",
        "deployment_custom = {\n",
        "    \"avatar_icon\": \"FaceActivated\",\n",
        "    \"avatar_color\": \"supportInfo\",\n",
        "    \"placeholder_image\": \"placeholder7.png\",\n",
        "    \"sample_questions\": [\"Start interview.\",\"Ask me questions for a fresher.\",\"Give me tips for answering questions.\",\"What are common mistakes in interviews?\"]\n",
        "}\n",
        "deployment_metadata = {\n",
        "    client.deployments.ConfigurationMetaNames.NAME: \"Interview partner buddy\",\n",
        "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
        "    client.deployments.ConfigurationMetaNames.CUSTOM: deployment_custom,\n",
        "    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"Hello, I&#x27;m an AI agent &quot;Interview partner&quot; and i will help you to get ready for your interview....\",\n",
        "    client.repository.AIServiceMetaNames.TAGS: [\"wx-agent\"]\n",
        "}\n",
        "\n",
        "function_deployment_details = client.deployments.create(ai_service_id, meta_props=deployment_metadata, space_id=space_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obeKh4C8XN5V"
      },
      "source": [
        "## 4. Test AI Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd6-HlNXN5V",
        "outputId": "ada617c3-bb13-48d8-d9cb-9adc3f49a1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0c52f4ed-456f-474d-85e2-bd78f381c8ec\n"
          ]
        }
      ],
      "source": [
        "# Get the ID of the AI Service deployment just created\n",
        "\n",
        "deployment_id = client.deployments.get_id(function_deployment_details)\n",
        "print(deployment_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aaDrTIjWXN5V"
      },
      "outputs": [],
      "source": [
        "messages = []\n",
        "remote_question = \"Change this question to test your function\"\n",
        "messages.append({ \"role\" : \"user\", \"content\": remote_question })\n",
        "payload = { \"messages\": messages }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "xqCJFQicXN5V",
        "outputId": "5a72604e-489f-41b9-c5de-9e8fb2acb49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ibm_watsonx_ai.wml_client_error:Failure during AI Service run. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/0c52f4ed-456f-474d-85e2-bd78f381c8ec/ai_service?version=2025-07-16)\n",
            "Status code: 400, body: {\"trace\":\"a914ae998c6e0f4e9f6a7089d79275a0\",\"errors\":[{\"code\":\"score_processing_failure\",\"message\":\"Failure during chat. (POST https://eu-gb.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-07)\\nStatus code: 429, body: {\\\"errors\\\":[{\\\"code\\\":\\\"consumption_limit_reached\\\",\\\"message\\\":\\\"The usage limit for the current plan has been reached: the total number of free concurrent requests for model meta-llama/llama-3-3-70b-instruct has reached its limit 10. Please try again later\\\",\\\"more_info\\\":\\\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\\\"}],\\\"trace\\\":\\\"c7e98308453be94b037b59abdc06dc97\\\",\\\"status_code\\\":429}\"}],\"status_code\":400}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ApiRequestFailure",
          "evalue": "Failure during AI Service run. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/0c52f4ed-456f-474d-85e2-bd78f381c8ec/ai_service?version=2025-07-16)\nStatus code: 400, body: {\"trace\":\"a914ae998c6e0f4e9f6a7089d79275a0\",\"errors\":[{\"code\":\"score_processing_failure\",\"message\":\"Failure during chat. (POST https://eu-gb.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-07)\\nStatus code: 429, body: {\\\"errors\\\":[{\\\"code\\\":\\\"consumption_limit_reached\\\",\\\"message\\\":\\\"The usage limit for the current plan has been reached: the total number of free concurrent requests for model meta-llama/llama-3-3-70b-instruct has reached its limit 10. Please try again later\\\",\\\"more_info\\\":\\\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\\\"}],\\\"trace\\\":\\\"c7e98308453be94b037b59abdc06dc97\\\",\\\"status_code\\\":429}\"}],\"status_code\":400}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3394651057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_ai_service\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/deployments.py\u001b[0m in \u001b[0;36mrun_ai_service\u001b[0;34m(self, deployment_id, ai_service_payload, path_suffix)\u001b[0m\n\u001b[1;32m   2136\u001b[0m             )\n\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AI Service run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_scoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m     def run_ai_service_stream(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ibm_watsonx_ai/wml_resource.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response, _silent_response_logging, _field_to_hide)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             raise ApiRequestFailure(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"Failure during {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperationName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during AI Service run. (POST https://eu-gb.ml.cloud.ibm.com/ml/v4/deployments/0c52f4ed-456f-474d-85e2-bd78f381c8ec/ai_service?version=2025-07-16)\nStatus code: 400, body: {\"trace\":\"a914ae998c6e0f4e9f6a7089d79275a0\",\"errors\":[{\"code\":\"score_processing_failure\",\"message\":\"Failure during chat. (POST https://eu-gb.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-07)\\nStatus code: 429, body: {\\\"errors\\\":[{\\\"code\\\":\\\"consumption_limit_reached\\\",\\\"message\\\":\\\"The usage limit for the current plan has been reached: the total number of free concurrent requests for model meta-llama/llama-3-3-70b-instruct has reached its limit 10. Please try again later\\\",\\\"more_info\\\":\\\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\\\"}],\\\"trace\\\":\\\"c7e98308453be94b037b59abdc06dc97\\\",\\\"status_code\\\":429}\"}],\"status_code\":400}"
          ]
        }
      ],
      "source": [
        "result = client.deployments.run_ai_service(deployment_id, payload)\n",
        "if \"error\" in result:\n",
        "    print(result[\"error\"])\n",
        "else:\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6zRpcPQXN5V"
      },
      "source": [
        "# Next steps\n",
        "You successfully deployed and tested the AI Service! You can now view\n",
        "your deployment and test it as a REST API endpoint.\n",
        "\n",
        "<a id=\"copyrights\"></a>\n",
        "### Copyrights\n",
        "\n",
        "Licensed Materials - Copyright © 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\n",
        "Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
        "\n",
        "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
        "\n",
        "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}